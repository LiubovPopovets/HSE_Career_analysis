{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printer(data):\n",
    "    \"\"\"\n",
    "    get: list of dictionaries\n",
    "    print: 'title' and 'link' from each dictionari\n",
    "    \"\"\"\n",
    "    for e in data:\n",
    "        print(e['title'])\n",
    "        print(e['href'])\n",
    "        \n",
    "        \n",
    "def finder(data, select):\n",
    "    \"\"\"\n",
    "    get: list of dictionaries\n",
    "    return: list of dictionaries minus sublist of dictionaries by selector,\n",
    "            sublist of dictionaries by selector\n",
    "    \"\"\"\n",
    "    run_date = data[0]\n",
    "    g = [run_date]\n",
    "    v = [run_date]\n",
    "    \n",
    "    for e in data[1:]:\n",
    "        t = e['title']\n",
    "        if (re.match(r'[ДПА][а-яВ\\s]+\\d{,2}[\\.\\s][\\dа-я]+[\\.\\s]\\d{,4}', t)\n",
    "            and select in t):\n",
    "            g.append(e)\n",
    "        else:\n",
    "            v.append(e)\n",
    "    return v, g\n",
    "\n",
    "\n",
    "def xa0(string):\n",
    "    \"\"\"\n",
    "    Hey, mister \\xa0, GTFOH! \n",
    "    \"\"\"\n",
    "    if '\\xa0' in string:\n",
    "        string = string.replace('\\xa0', '')\n",
    "    return string\n",
    "\n",
    "\n",
    "def check_keys(dictofdicts):\n",
    "    \"\"\"\n",
    "    get: dictionari of dictionaries\n",
    "    print: keys of dictionaries\n",
    "    \"\"\"\n",
    "    q = []\n",
    "    for k, v in w.items():\n",
    "        for i, l in v.items():\n",
    "            q.append(i)\n",
    "    print(set(q))\n",
    "    \n",
    "    \n",
    "def get_date(yyyy, mm, dd):\n",
    "    \"\"\"\n",
    "    get: mess\n",
    "    return: DateTime format\n",
    "    \"\"\"\n",
    "    mm_eng = {'Янв':'Jan', 'Фев':'Feb', 'Мар':'Mar', 'Апр':'Apr', \n",
    "              'Мая':'May', 'Июн':'Jun', 'Июл':'Jul', 'Авг':'Aug', \n",
    "              'Сен':'Sep', 'Окт':'Oct', 'Ноя':'Nov', 'Дек':'Dec'}\n",
    "    \n",
    "    g = dd+'/'+mm_eng[mm.capitalize()]+'/'+yyyy\n",
    "    d = datetime.datetime.strptime(g, '%d/%b/%Y').date()\n",
    "    return d\n",
    "\n",
    "\n",
    "def DieWalkuere():\n",
    "    \"\"\"\n",
    "    return: json from career.hse.ru/news\n",
    "    \"\"\"\n",
    "    run_datetime = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    m_r = requests.get('https://career.hse.ru/news/') # делаем запрос к сайту\n",
    "    soup = bs(m_r.text, 'lxml')\n",
    "    num_of_cycles = int(soup.find_all('a', {'class' : 'pages__page'})[-1].text) # узнаём количество страниц в новостной ленте\n",
    "    hse_career_posts = [run_datetime] # список в который будут записаны результаты\n",
    "\n",
    "    with tqdm(total=num_of_cycles) as pbar:\n",
    "        for i in range(num_of_cycles): # создаём цикл, с количеством итераций равным количеству страниц в новостной ленте\n",
    "            r = requests.get(f'https://career.hse.ru/news/page{i}.html') # запрошиваем нужную страницу\n",
    "            soup = bs(r.text, 'lxml') # применяем к ее html BeautifulSoup, преобразуем в lxml\n",
    "\n",
    "            posts_content = soup.find_all(\"div\", {\"class\": \"post__content\"}) # вычленяем все div'ы в которых содежраться новостные посты\n",
    "            posts_meta = soup.find_all(\"div\", {\"class\": \"post-meta__date\"}) # вычленяем div'ы с мета-данными постов\n",
    "\n",
    "            qq = list(zip(posts_content, posts_meta)) # объеденяем списоки div'ов с постами и div'ов с мета-данными в список кортежей\n",
    "\n",
    "            for e in qq:\n",
    "                title = e[0].find('a').text # заголовок поста\n",
    "                href = e[0].find('a')['href'] # ссылка на страницу с конкретной новостью\n",
    "                text_content = e[0].find('div', {'class' : 'post__text'}).text # лид из поста\n",
    "\n",
    "                # из элемента с мета-данными получаем год, месяц и день, записанные в очень своеобразном формате\n",
    "                dd = e[1].find('div', {'class' : 'post-meta__day'}).text \n",
    "                mm = e[1].find('div', {'class' : 'post-meta__month'}).text\n",
    "                yyyy = e[1].find('div', {'class' : 'post-meta__year'}).text\n",
    "\n",
    "                d = str(get_date(yyyy, mm, dd)) # преобразуем дату в стандарт iso\n",
    "\n",
    "                e_dict = {'title':title, 'href':href, \n",
    "                          'text_content':text_content, 'date':d} # записываем все полученыеданне в словарь\n",
    "                hse_career_posts.append(e_dict) # помещаем словарь в список\n",
    "            pbar.update()\n",
    "            \n",
    "    with tqdm(total=len(hse_career_posts[1:])) as pbar:\n",
    "        for e in hse_career_posts[1:]:\n",
    "            try:\n",
    "                url = e['href']\n",
    "                if 'http:' in url:\n",
    "                    url = url.replace('http:', 'https:')\n",
    "                if not 'https:' in url:\n",
    "                    url = 'https:'+url\n",
    "                r = requests.get(url).text\n",
    "                e['href_content_html'] = r\n",
    "            except:\n",
    "                e['href_content_html'] = ''\n",
    "                pass\n",
    "            pbar.update()\n",
    "            \n",
    "    return hse_career_posts\n",
    "\n",
    "\n",
    "def Fricka(listofobj):\n",
    "    \"\"\"\n",
    "    get: list of dictionaries\n",
    "    return: list of strings + run_date\n",
    "    \"\"\"\n",
    "    title_p = \"margin-bottom: .0001pt; text-align: center; text-indent: 1.0cm; line-height: 150%;\"\n",
    "    Liste = [listofobj[0]]\n",
    "    with tqdm(total=len(listofobj[1:])) as pbar:\n",
    "        for obj in listofobj[1:]:\n",
    "            href = obj.get('href_content_html') # html код страницы с дайджестом\n",
    "            o = bs(href, 'lxml') # применяем BeautifulSoup, преобразуем в lxml\n",
    "            core_div = o.find('div', {'class': 'post__text'}) \n",
    "            list_of_p = core_div.find_all('p') # вычленяем блоки с текстами вакансий\n",
    "            for p in list_of_p:\n",
    "                if p.get('style') == title_p: # находим блоки в которых заголовки вакансий\n",
    "                    tex = p.text\n",
    "                    tex = xa0(tex) #удаляем неразрывные пробелы\n",
    "                    if tex != '':\n",
    "                        Liste.append('o'+tex) # добавляем в начало символьный маркер для заголовка вакансии\n",
    "                        Liste.append(obj['date']) # добавляем текст блока в список\n",
    "                else:\n",
    "                    spans = p.find_all('span') # находим в блоке все span'ы\n",
    "                    for s in spans:\n",
    "                        tex = s.text # достаём из каждого текст\n",
    "                        tex = xa0(tex) # удаляем неразрывные пробелы\n",
    "                        if tex != '':\n",
    "                            Liste.append(tex) # добавляем в список\n",
    "            pbar.update()\n",
    "    return Liste\n",
    "\n",
    "\n",
    "def Wotan(listofstr):\n",
    "    \"\"\"\n",
    "    get: list of strings\n",
    "    return: list: run_date and dictionari of dictionaries\n",
    "            example: {title_1 : {aspect_1 : text_1, aspect_2 : text_2}, \n",
    "                      title_2 : {aspect_1 : text_1, aspect_2 : text_2},\n",
    "                      ...\n",
    "                      }\n",
    "    \"\"\"\n",
    "    \n",
    "    keys = ({'Контакты:':'contacts', 'О компании:':'about_company', \n",
    "             'Обязанности:':'responsibilities', 'Условия:':'conditions', \n",
    "             'Окомпании:':'about_company', 'Требования:':'demands'}) # названия подзаголовков\n",
    "    WW = [listofstr[0]]\n",
    "    Woerterbuch = defaultdict(dict) # структура словарь словарей\n",
    "    current_date = ''\n",
    "    current_title = ''\n",
    "    curretn_aspect = ''\n",
    "    \n",
    "    with tqdm(total=len(listofstr[1:])) as pbar:\n",
    "        # заголовок вакансии – ключ, его значение – словарь, ключи в котором аспекты каждой вакансии\n",
    "        for i, e in enumerate(listofstr[1:]):\n",
    "            t = re.compile(r'[o]\\d+[\\.][\\s]\\b') # сохраняем паттерн для вычленения заголовкой\n",
    "            if t.match(e): # распознаём заголовок\n",
    "                nomore = t.match(e).group() \n",
    "                current_title = e.replace(nomore, '')\n",
    "                Woerterbuch[current_title]\n",
    "            try:   \n",
    "                if re.match(r'\\d{4}[-]\\d{2}[-]\\d{2}\\b', e): # распознаем дату\n",
    "                    current_date = e\n",
    "                    Woerterbuch[current_title]['date'] = current_date\n",
    "                elif e.strip() in keys: # распознаем аспекты вакании\n",
    "                    curretn_aspect = keys[e.strip()]\n",
    "                    Woerterbuch[current_title][curretn_aspect] = ''\n",
    "                else: # всё остальное – содержание аспектов\n",
    "                    Woerterbuch[current_title][curretn_aspect]+=e\n",
    "            except:\n",
    "                pass\n",
    "            pbar.update()\n",
    "        WW.append(Woerterbuch)\n",
    "    return WW\n",
    "\n",
    "\n",
    "def Bruennhilde(dictofdicts):\n",
    "    \"\"\"\n",
    "    get: dictionary of dictionaries\n",
    "            example: {title_1 : {aspect_1 : text_1, aspect_2 : text_2}, \n",
    "                      title_2 : {aspect_1 : text_1, aspect_2 : text_2},\n",
    "                      ...\n",
    "                      }\n",
    "    return: list of dictionaries\n",
    "            example: [{title : title_1, aspect_1 : text_1, aspect_2 : text_2}, \n",
    "                      {title : title_2, aspect_1 : text_1, aspect_2 : text_2},\n",
    "                      ...\n",
    "                      ]\n",
    "    \"\"\"\n",
    "    with open('metro.txt', 'r', encoding='utf-8') as f:\n",
    "        ms = f.read().split('\\n')\n",
    "    \n",
    "    Siegfried = []\n",
    "    # меняем структуру данных для простоты табличного представления, и достаём дополнительные данные\n",
    "    for k, v in dictofdicts.items():\n",
    "        Fafnir = {}\n",
    "        c = k.split(' в ')\n",
    "        n = re.compile(r'[А-ЯЁA-Z][\\w\\s]+')\n",
    "        # вычленяем название компании из заговоловка вакансии\n",
    "        if len(c) > 1 and n.search(c[-1]):\n",
    "            Fafnir['title'] = k\n",
    "            Fafnir['company'] = n.search(c[-1]).group()\n",
    "        else:\n",
    "            Fafnir['title'] = k\n",
    "        money = re.compile(r'[\\d][\\d\\s]+[руб]{1,3}[.]{,1}\\s')\n",
    "        metrore = re.compile(r'\\s[м][.][\\s][А-ЯЁ][а-я]+\\s')\n",
    "        # пытаемся найти информацию о деньгах и локациях (относительно метро)\n",
    "        if 'conditions' in v:\n",
    "            cond = v['conditions']\n",
    "            if money.search(cond):\n",
    "                Fafnir['money'] = ', '.join(money.findall(cond))\n",
    "            metro = []\n",
    "            if metrore.search(cond):\n",
    "                for e in ms:\n",
    "                    if e in cond:\n",
    "                        metro.append('м. ' + e)\n",
    "                if len(metro) > 0:\n",
    "                    Fafnir['metro'] = ', '.join(metro)\n",
    "            Fafnir.update(v)\n",
    "        if len(Fafnir) > 2:\n",
    "            Siegfried.append(Fafnir)\n",
    "    return Siegfried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [01:00<00:00,  1.26s/it]\n",
      "100%|██████████| 459/459 [07:11<00:00,  1.81s/it]\n"
     ]
    }
   ],
   "source": [
    "#This cell downloads the website, be careful!\n",
    "\n",
    "Data = DieWalkuere()\n",
    "\n",
    "with open('hse_career_department_website_posts.json', 'w') as f:\n",
    "        json.dump(Data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hse_career_department_website_posts.json', 'r') as f:\n",
    "        Data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:08<00:00, 10.46it/s]\n",
      "100%|██████████| 30918/30918 [00:00<00:00, 143512.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# данные о вакансиях для студентов\n",
    "\n",
    "data2, stud = finder(Data, 'студент')\n",
    "\n",
    "\n",
    "step1_stud = Fricka(stud)\n",
    "step2_stud = Wotan(step1_stud)\n",
    "step3_stud = Bruennhilde(step2_stud[1])\n",
    "\n",
    "Table_stud = pandas.DataFrame(step3_stud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:08<00:00,  9.91it/s]\n",
      "100%|██████████| 23908/23908 [00:00<00:00, 138908.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# данные о вакансиях для выпускников\n",
    "\n",
    "data3, grad = finder(data2, 'выпускник')\n",
    "\n",
    "step1_grad = Fricka(grad)\n",
    "step2_grad = Wotan(step1_grad)\n",
    "step3_grad = Bruennhilde(step2_grad[1])\n",
    "\n",
    "Table_grad = pandas.DataFrame(step3_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Table_stud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Table_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраня\n",
    "\n",
    "rdd = 'results'+step2_stud[0]\n",
    "\n",
    "os.mkdir(rdd)\n",
    "\n",
    "with open(rdd+'\\Table_stud'+step2_stud[0]+'.json', 'w') as f:\n",
    "        json.dump(step3_stud, f)\n",
    "Table_stud.to_csv(rdd+'\\Table_stud'+step2_stud[0]+'.csv', encoding='utf-8')\n",
    "\n",
    "\n",
    "with open(rdd+'\\Table_grad'+step2_grad[0]+'.json', 'w') as f:\n",
    "        json.dump(step3_grad, f)\n",
    "Table_grad.to_csv(rdd+'\\Table_grad'+step2_grad[0]+'.csv', encoding='utf-8')\n",
    "\n",
    "\n",
    "with open(rdd+'\\ostatki'+step2_grad[0]+'.json', 'w') as f:\n",
    "        json.dump(data3, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

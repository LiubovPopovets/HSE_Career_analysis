{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printer(data):\n",
    "    \"\"\"\n",
    "    get: list of dictionaries\n",
    "    print: 'title' and 'link' from each dictionari\n",
    "    \"\"\"\n",
    "    for e in data:\n",
    "        print(e['title'])\n",
    "        print(e['href'])\n",
    "        \n",
    "        \n",
    "def finder(data, select):\n",
    "    \"\"\"\n",
    "    get: list of dictionaries\n",
    "    return: list of dictionaries minus sublist of dictionaries by selector,\n",
    "            sublist of dictionaries by selector\n",
    "    \"\"\"\n",
    "    g = []\n",
    "    v = []\n",
    "    \n",
    "    for e in data:\n",
    "        t = e['title']\n",
    "        if (re.match(r'[ДПА][а-яВ\\s]+\\d{,2}[\\.\\s][\\dа-я]+[\\.\\s]\\d{,4}', t)\n",
    "            #юшпотВиыдлрквнеьйаугямжс\n",
    "            and select in t):\n",
    "            g.append(e)\n",
    "        else:\n",
    "            v.append(e)\n",
    "    return v, g\n",
    "\n",
    "\n",
    "def xa0(string):\n",
    "    \"\"\"\n",
    "    Hey, mister \\xa0, GTFOH! \n",
    "    \"\"\"\n",
    "    if '\\xa0' in string:\n",
    "        string = string.replace('\\xa0', '')\n",
    "    return string\n",
    "\n",
    "\n",
    "def check_keys(dictofdicts):\n",
    "    \"\"\"\n",
    "    get: dictionari of dictionaries\n",
    "    print: keys of dictionaries\n",
    "    \"\"\"\n",
    "    q = []\n",
    "    for k, v in w.items():\n",
    "        for i, l in v.items():\n",
    "            q.append(i)\n",
    "    print(set(q))\n",
    "    \n",
    "    \n",
    "def get_date(yyyy, mm, dd):\n",
    "    \"\"\"\n",
    "    get: mess\n",
    "    return: DateTime format\n",
    "    \"\"\"\n",
    "    mm_eng = {'Янв':'Jan', 'Фев':'Feb', 'Мар':'Mar', 'Апр':'Apr', \n",
    "              'Мая':'May', 'Июн':'Jun', 'Июл':'Jul', 'Авг':'Aug', \n",
    "              'Сен':'Sep', 'Окт':'Oct', 'Ноя':'Nov', 'Дек':'Dec'}\n",
    "    \n",
    "    g = dd+'/'+mm_eng[mm.capitalize()]+'/'+yyyy\n",
    "    d = datetime.datetime.strptime(g, '%d/%b/%Y').date()\n",
    "    return d\n",
    "\n",
    "\n",
    "def DieWalkuere():\n",
    "    \"\"\"\n",
    "    return: json from career.hse.ru/news\n",
    "    \"\"\"\n",
    "\n",
    "    m_r = requests.get('https://career.hse.ru/news/')\n",
    "    soup = bs(m_r.text, 'lxml')\n",
    "    num_of_cycles = int(soup.find_all('a', {'class' : 'pages__page'})[-1].text)\n",
    "    hse_career_posts = []\n",
    "\n",
    "    with tqdm(total=num_of_cycles) as pbar:\n",
    "        for i in range(num_of_cycles):\n",
    "            r = requests.get(f'https://career.hse.ru/news/page{i}.html')\n",
    "            soup = bs(r.text, 'lxml')\n",
    "\n",
    "            posts_content = soup.find_all(\"div\", {\"class\": \"post__content\"})\n",
    "            posts_meta = soup.find_all(\"div\", {\"class\": \"post-meta__date\"})\n",
    "\n",
    "            qq = list(zip(posts_content, posts_meta))\n",
    "\n",
    "            for e in qq:\n",
    "                title = e[0].find('a').text\n",
    "                href = e[0].find('a')['href']\n",
    "                text_content = e[0].find('div', {'class' : 'post__text'}).text\n",
    "\n",
    "                dd = e[1].find('div', {'class' : 'post-meta__day'}).text\n",
    "                mm = e[1].find('div', {'class' : 'post-meta__month'}).text\n",
    "                yyyy = e[1].find('div', {'class' : 'post-meta__year'}).text\n",
    "\n",
    "                d = str(get_date(yyyy, mm, dd))\n",
    "\n",
    "                e_dict = {'title':title, 'href':href, \n",
    "                          'text_content':text_content, 'date':d}\n",
    "                hse_career_posts.append(e_dict)\n",
    "            pbar.update() \n",
    "\n",
    "    with tqdm(total=len(hse_career_posts)) as pbar:\n",
    "        for e in hse_career_posts:\n",
    "            try:\n",
    "                url = e['href']\n",
    "                if 'http:' in url:\n",
    "                    url = url.replace('http:', 'https:')\n",
    "                if not 'https:' in url:\n",
    "                    url = 'https:'+url\n",
    "                r = requests.get(url).text\n",
    "                e['href_content_html'] = r\n",
    "            except:\n",
    "                e['href_content_html'] = ''\n",
    "                pass\n",
    "            pbar.update()\n",
    "    return hse_career_posts\n",
    "\n",
    "\n",
    "def Fricka(listofobj):\n",
    "    \"\"\"\n",
    "    get: list of dictionaries\n",
    "    return: list of strings\n",
    "    \"\"\"\n",
    "    title_p = \"margin-bottom: .0001pt; text-align: center; text-indent: 1.0cm; line-height: 150%;\"\n",
    "    Liste = []\n",
    "    with tqdm(total=len(listofobj)) as pbar:\n",
    "        for obj in listofobj:\n",
    "            href = obj.get('href_content_html')\n",
    "            o = bs(href, 'lxml')\n",
    "            core_div = o.find('div', {'class': 'post__text'})\n",
    "            list_of_p = core_div.find_all('p')\n",
    "            for p in list_of_p:\n",
    "                if p.get('style') == title_p:\n",
    "                    tex = p.text\n",
    "                    tex = xa0(tex)\n",
    "                    if tex != '':\n",
    "                        Liste.append('o'+tex)\n",
    "                        Liste.append(obj['date'])\n",
    "                else:\n",
    "                    spans = p.find_all('span')\n",
    "                    for s in spans:\n",
    "                        tex = s.text\n",
    "                        tex = xa0(tex)\n",
    "                        if tex != '':\n",
    "                            Liste.append(tex)\n",
    "            pbar.update()\n",
    "    return Liste\n",
    "\n",
    "\n",
    "def Wotan(listofstr):\n",
    "    \"\"\"\n",
    "    get: list of strings\n",
    "    return: dictionari of dictionaries\n",
    "            example: {title_1 : {aspect_1 : text_1, aspect_2 : text_2}, \n",
    "                      title_2 : {aspect_1 : text_1, aspect_2 : text_2},\n",
    "                      ...\n",
    "                      }\n",
    "    \"\"\"\n",
    "    \n",
    "    keys = ['Контакты:', 'О компании:', 'Обязанности:', 'Условия:', 'Окомпании:', 'Требования:']\n",
    "    Woerterbuch = defaultdict(dict)\n",
    "    current_date = ''\n",
    "    current_title = ''\n",
    "    curretn_aspect = ''\n",
    "    \n",
    "    with tqdm(total=len(listofstr)) as pbar:\n",
    "        for i, e in enumerate(listofstr):\n",
    "            t = re.compile(r'[o]\\d+[\\.][\\s]\\b')\n",
    "            if t.match(e):\n",
    "                nomore = t.match(e).group()\n",
    "                current_title = e.replace(nomore, '')\n",
    "                Woerterbuch[current_title]\n",
    "                #print(i, current_title)\n",
    "            try:   \n",
    "                if re.match(r'\\d{4}[-]\\d{2}[-]\\d{2}\\b', e):\n",
    "                    current_date = e\n",
    "                    Woerterbuch[current_title]['date'] = current_date\n",
    "                    #print(i, current_date)\n",
    "                elif e.strip() in keys:\n",
    "                    curretn_aspect = e.strip()\n",
    "                    if curretn_aspect == 'Окомпании:':\n",
    "                        curretn_aspect = 'О компании:'\n",
    "                    Woerterbuch[current_title][curretn_aspect] = ''\n",
    "                    #print(i, curretn_aspect)\n",
    "                else:\n",
    "                    Woerterbuch[current_title][curretn_aspect]+=e\n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "            pbar.update() \n",
    "    return Woerterbuch\n",
    "\n",
    "\n",
    "def Bruennhilde(dictofdicts):\n",
    "    \"\"\"\n",
    "    get: dictionary of dictionaries\n",
    "            example: {title_1 : {aspect_1 : text_1, aspect_2 : text_2}, \n",
    "                      title_2 : {aspect_1 : text_1, aspect_2 : text_2},\n",
    "                      ...\n",
    "                      }\n",
    "    return: list of dictionaries\n",
    "            example: [{title : title_1, aspect_1 : text_1, aspect_2 : text_2}, \n",
    "                      {title : title_2, aspect_1 : text_1, aspect_2 : text_2},\n",
    "                      ...\n",
    "                      ]\n",
    "    \"\"\"\n",
    "    with open('metro.txt', 'r', encoding='utf-8') as f:\n",
    "        ms = f.read().split('\\n')\n",
    "    \n",
    "    Siegfried = []\n",
    "    for k, v in dictofdicts.items():\n",
    "        Fafnir = {}\n",
    "        c = k.split(' в ')\n",
    "        n = re.compile(r'[А-ЯЁA-Z][\\w\\s]+')\n",
    "        if len(c) > 1 and n.search(c[-1]):\n",
    "            Fafnir['title'] = k\n",
    "            Fafnir['company'] = n.search(c[-1]).group()\n",
    "        else:\n",
    "            Fafnir['title'] = k\n",
    "        money = re.compile(r'[\\d][\\d\\s]+[руб]{1,3}[.]{,1}\\s')\n",
    "        metrore = re.compile(r'\\s[м][.][\\s][А-ЯЁ][а-я]+\\s')\n",
    "        if 'Условия:' in v:\n",
    "            cond = v['Условия:']\n",
    "            if money.search(cond):\n",
    "                Fafnir['money'] = ', '.join(money.findall(cond))\n",
    "            metro = []\n",
    "            if metrore.search(cond):\n",
    "                for e in ms:\n",
    "                    if e in cond:\n",
    "                        metro.append('м. ' + e)\n",
    "                if len(metro) > 0:\n",
    "                    Fafnir['metro'] = ', '.join(metro)\n",
    "            Fafnir.update(v)\n",
    "        Siegfried.append(Fafnir)\n",
    "    return Siegfried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell downloads the website, be careful!\n",
    "\n",
    "Data = DieWalkuere()\n",
    "\n",
    "with open('hse_career_department_website_posts.json', 'w') as f:\n",
    "        json.dump(Data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hse_career_department_website_posts.json', 'r') as f:\n",
    "        Data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2, stud = finder(Data, 'студент')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3, grad = finder(data2, 'выпускник')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 89/89 [00:08<00:00, 14.15it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 30505/30505 [00:00<00:00, 147236.39it/s]\n"
     ]
    }
   ],
   "source": [
    "step1_stud = Fricka(stud)\n",
    "step2_stud = Wotan(step1_stud)\n",
    "step3_stud = Bruennhilde(step2_stud)\n",
    "\n",
    "Table_stud = pandas.DataFrame(step3_stud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 82/82 [00:07<00:00, 10.49it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 22616/22616 [00:00<00:00, 136976.25it/s]\n"
     ]
    }
   ],
   "source": [
    "step1_grad = Fricka(grad)\n",
    "step2_grad = Wotan(step1_grad)\n",
    "step3_grad = Bruennhilde(step2_grad)\n",
    "\n",
    "Table_grad = pandas.DataFrame(step3_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Table_stud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Table_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('results')\n",
    "\n",
    "with open('results\\Table_stud.json', 'w') as f:\n",
    "        json.dump(data3, f)\n",
    "Table_stud.to_csv('results\\Table_stud.csv', encoding='utf-8')\n",
    "\n",
    "\n",
    "with open('results\\Table_grad.json', 'w') as f:\n",
    "        json.dump(data3, f)\n",
    "Table_grad.to_csv('results\\Table_grad.csv', encoding='utf-8')\n",
    "\n",
    "\n",
    "with open('results\\ostatki.json', 'w') as f:\n",
    "        json.dump(data3, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
